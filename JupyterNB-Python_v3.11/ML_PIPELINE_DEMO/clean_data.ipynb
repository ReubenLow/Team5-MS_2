{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef05f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "import argparse\n",
    "import configparser\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', None)  # Display all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d7de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to open config file for review\n",
    "def open_config_file(config_file):\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuration file '{config_file}' not found. Please make sure it exists.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"Opening configuration file '{config_file}' for review...\")\n",
    "        subprocess.Popen(['open' if os.name == 'posix' else 'start', config_file], shell=True)\n",
    "        input(\"Press Enter when you're ready to proceed with data cleaning...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open the configuration file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44cba3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to load paths and suffix from config file\n",
    "def load_paths_and_suffix(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    paths = {\n",
    "        \"input_folder\": config[\"Paths\"].get(\"input_folder_clean\", \"../OTH_DATA/training_data\"),\n",
    "        \"output_folder\": config[\"Paths\"].get(\"output_folder_clean\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"cleaned_file_suffix\": config[\"Paths\"].get(\"cleaned_file_suffix\", \"_v1\")\n",
    "    }\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data function\n",
    "def clean_data(data, drop_columns=None, add_target=False, target_column_name=\"target\"):\n",
    "    # Drop unnecessary columns\n",
    "    # If there is extra trailing delimiter, pandas will create an extra column 'Unnamed: 18'\n",
    "    if 'Unnamed: 18' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 18'])\n",
    "\n",
    "    # Drop user-specified columns\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns, errors='ignore')\n",
    "        print(f\"Dropped columns: {drop_columns}\")\n",
    "\n",
    "    # Handle missing values, fill numerical columns with median\n",
    "    for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            data[column].fillna(data[column].median(), inplace=True)\n",
    "            \n",
    "    # Calculate BMI if Height and Weight columns are present\n",
    "    if 'Height' in data.columns and 'Weight' in data.columns:\n",
    "        data['BMI'] = data['Weight'] / ((data['Height']) ** 2)\n",
    "        data['BMI'] = data['BMI'].round(2)\n",
    "\n",
    "    # Encoding categorical variables with numbers\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        data[column] = data[column].astype('category').cat.codes\n",
    "\n",
    "    # Add an empty target column if requested for dataset with no target column\n",
    "    if add_target and target_column_name not in data.columns:\n",
    "        data[target_column_name] = None\n",
    "        print(f\"Added an empty target column: '{target_column_name}'\")\n",
    "\n",
    "    print(\"Data cleaning complete.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c5d38",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#function to display mapped encoded numbers to previous said values\n",
    "def generate_encoding_summary(original_data, encoded_data):\n",
    "    \"\"\"\n",
    "    Generates a summary of the encoding performed on categorical variables.\n",
    "\n",
    "    Parameters:\n",
    "        original_data (pd.DataFrame): The original DataFrame before encoding.\n",
    "        encoded_data (pd.DataFrame): The DataFrame after encoding.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the mapping for each categorical column.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    categorical_columns = original_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        # Check if the column exists in both original and encoded data\n",
    "        if column in original_data.columns and column in encoded_data.columns:\n",
    "            # Create a mapping of categories to codes\n",
    "            original_col = original_data[column].astype('category')\n",
    "            mapping = dict(enumerate(original_col.cat.categories))\n",
    "            summary[column] = mapping\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoding_summary(encoding_summary):\n",
    "    \"\"\"\n",
    "    Prints the encoding summary in a readable format.\n",
    "\n",
    "    Parameters:\n",
    "        encoding_summary (dict): The summary dictionary with mappings.\n",
    "    \"\"\"\n",
    "    for column, mapping in encoding_summary.items():\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        for code, category in mapping.items():\n",
    "            print(f\"  {code} -> {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f31094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define paths and configuration\n",
    "config_file = \"config.txt\"\n",
    "\n",
    "# Step 1: Open the configuration file for review\n",
    "if not open_config_file(config_file):\n",
    "    print(\"Exiting due to missing or inaccessible config file.\")\n",
    "else:\n",
    "    # Step 2: Load paths and suffix\n",
    "    paths = load_paths_and_suffix(config_file)\n",
    "    input_folder = paths[\"input_folder\"]\n",
    "    output_folder = paths[\"output_folder\"]\n",
    "    cleaned_file_suffix = paths[\"cleaned_file_suffix\"]\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Step 3: List available files in the input folder\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        print(\"No files found in the input folder.\")\n",
    "    else:\n",
    "        print(\"\\nAvailable files:\")\n",
    "        for i, file in enumerate(files, 1):\n",
    "            print(f\"{i}. {file}\")\n",
    "\n",
    "        # Step 4: Prompt user to select a file to clean\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number of the file you want to clean: \"))\n",
    "            if 1 <= choice <= len(files):\n",
    "                selected_file = files[choice - 1]\n",
    "            else:\n",
    "                print(\"Invalid selection.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "        input_path = os.path.join(input_folder, selected_file)\n",
    "        data = pd.read_csv(input_path)\n",
    "\n",
    "        # Step 5: Ask for columns to drop\n",
    "        user_input = input(\"Enter the columns you want to drop, separated by commas, or press Enter to skip: \")\n",
    "        drop_columns = [\"Patient ID\"]\n",
    "        if user_input:\n",
    "            drop_columns += [col.strip() for col in user_input.split(\",\")]\n",
    "\n",
    "        # Step 6: Ask if an empty target column should be added\n",
    "        add_target = input(\"Do you want to add an empty target column to this dataset? (yes/no): \").strip().lower() == \"yes\"\n",
    "        target_column_name = \"target\"\n",
    "        if add_target:\n",
    "            target_column_name = input(\"Enter the name of the target column: \")\n",
    "\n",
    "        # Step 7: Clean the data\n",
    "        cleaned_data = clean_data(data, drop_columns=drop_columns, add_target=add_target, target_column_name=target_column_name)\n",
    "\n",
    "        # Step 8: Save the cleaned data\n",
    "        output_filename = f\"cleaned_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        cleaned_data.to_csv(output_path, index=False)\n",
    "        print(f\"Saved cleaned data to {output_path}\")\n",
    "\n",
    "        # Step 9: Split data into train/test/validation sets (optional)\n",
    "        split_data = input(\"Do you want to split the data into training and testing sets? (yes/no): \").strip().lower() == \"yes\"\n",
    "        if split_data:\n",
    "            try:\n",
    "                train_percentage = float(input(\"Enter the percentage for training data (e.g., 80 for 80%): \")) / 100\n",
    "                validation_percentage = float(input(\"Enter the percentage for validation data (e.g., 10 for 10%): \")) / 100\n",
    "                test_percentage = float(input(\"Enter the percentage for test data (e.g., 10 for 10%): \")) / 100\n",
    "\n",
    "                # Ensure percentages add up to 1 (100%)\n",
    "                if not abs(train_percentage + validation_percentage + test_percentage - 1) < 1e-5:\n",
    "                    raise ValueError(\"Percentages must add up to 100%!\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Invalid input: {e}\")\n",
    "                print(\"Using default split: 70% train, 15% validation, 15% test.\")\n",
    "                train_percentage, validation_percentage, test_percentage = 0.7, 0.15, 0.15\n",
    "\n",
    "            if validation_percentage == 0:\n",
    "                # Split into training and testing only\n",
    "                train_data, test_data = train_test_split(cleaned_data, test_size=test_percentage, random_state=42)\n",
    "                validation_data = None\n",
    "            else:\n",
    "                # Split into training, validation, and testing\n",
    "                train_data, temp_data = train_test_split(cleaned_data, test_size=(validation_percentage + test_percentage), random_state=42)\n",
    "                validation_data, test_data = train_test_split(temp_data, test_size=(test_percentage / (validation_percentage + test_percentage)), random_state=42)\n",
    "\n",
    "            # Save the split datasets\n",
    "            train_output_path = os.path.join(output_folder, f\"train_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "            test_output_path = os.path.join(output_folder, f\"test_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "            train_data.to_csv(train_output_path, index=False)\n",
    "            test_data.to_csv(test_output_path, index=False)\n",
    "            print(f\"Saved training data to {train_output_path}\")\n",
    "            print(f\"Saved testing data to {test_output_path}\")\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_output_path = os.path.join(output_folder, f\"validation_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                validation_data.to_csv(validation_output_path, index=False)\n",
    "                print(f\"Saved validation data to {validation_output_path}\")\n",
    "\n",
    "        # Step 10: Generate and print encoding summary\n",
    "        summary = generate_encoding_summary(data, cleaned_data)\n",
    "        print_encoding_summary(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Clean specific data file in the training_data folder.\")\n",
    "    parser.add_argument('--config_file', type=str, default=\"config.txt\", help=\"Path to the configuration file.\")\n",
    "    parser.add_argument('--file', type=str, default=None, help=\"Specific file to clean.\")\n",
    "    args = parser.parse_args()\n",
    "    main(args.config_file, args.file)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
