{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9820bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import argparse\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn import neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# from sklearn.linear_model import LogisticRegression, Ridge, Perceptron, SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import configparser\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b056e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# config_file = \"config.txt\"\n",
    "def open_config_file(config_file):\n",
    "    # Check if the config file exists\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuration file '{config_file}' not found. Please make sure it exists.\")\n",
    "        return False\n",
    "    \n",
    "    # Open the config file in the default text editor\n",
    "    try:\n",
    "        print(f\"Opening configuration file '{config_file}' for review...\")\n",
    "        subprocess.Popen(['open' if os.name == 'posix' else 'start', config_file], shell=True)\n",
    "        input(\"Press Enter when you're ready to proceed with model training...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open the configuration file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168e54b",
   "metadata": {},
   "source": [
    "def load_model_params(config_file, model_name):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab1beb",
   "metadata": {},
   "source": [
    "    def safe_eval(value):\n",
    "        # Attempt to evaluate the value, return as a string if it fails\n",
    "        try:\n",
    "            return eval(value)\n",
    "        except NameError:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Return the parsed and safely evaluated parameters\n",
    "#     return {key: safe_eval(value) for key, value in config[model_name].items()}\n",
    "# Load model parameters\n",
    "def load_model_params(config_file, model_name):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    params = {}\n",
    "    for key, value in config[model_name].items():\n",
    "        try:\n",
    "            # Try to evaluate value as Python literal if possible\n",
    "            params[key] = eval(value)\n",
    "        except (NameError, SyntaxError):\n",
    "            # If eval fails, treat as a string (e.g., \"l2\" for penalty)\n",
    "            params[key] = value.strip(\"'\\\"\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad60611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths and suffix\n",
    "def load_paths_and_suffix(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    paths = {\n",
    "        \"input_folder\": config[\"Paths\"].get(\"input_folder\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"input_file\": config[\"Paths\"].get(\"input_file\", \"default.csv\"),\n",
    "        \"output_folder\": config[\"Paths\"].get(\"output_folder\", \"../ML_DATA/model_outputs\"),\n",
    "        \"output_file\": config[\"Paths\"].get(\"output_file\", \"default_model.pkl\"),\n",
    "        \"model_name_suffix\": config[\"Paths\"].get(\"model_name_suffix\", \"_v1\")\n",
    "    }\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab9e3b",
   "metadata": {},
   "source": [
    "def train_and_save_models(X, y, model_output, models):\n",
    "    if not os.path.exists(model_output):\n",
    "        os.makedirs(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b58aec",
   "metadata": {},
   "source": [
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ad36e",
   "metadata": {},
   "source": [
    "    # Create a column transformer to handle both categorical and numeric columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccbafd",
   "metadata": {},
   "source": [
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),  # Preprocess categorical and numeric data\n",
    "            ('model', model)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d9912",
   "metadata": {},
   "source": [
    "        # Fit the pipeline with preprocessed data\n",
    "        pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1fef3",
   "metadata": {},
   "source": [
    "        # Extract feature names after preprocessing\n",
    "        feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "        pipeline.feature_names = feature_names  # Save feature names to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a177fa0",
   "metadata": {},
   "source": [
    "        # Save the trained model\n",
    "        model_path = os.path.join(model_output, f\"{model_name}_model.pkl\")\n",
    "        joblib.dump(pipeline, model_path)\n",
    "        print(f\"Trained and saved model: {model_name} to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8ba17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train and save selected models\n",
    "def train_and_save_models(X, y, paths, models):\n",
    "    if not os.path.exists(paths[\"output_folder\"]):\n",
    "        os.makedirs(paths[\"output_folder\"])\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X, y)\n",
    "\n",
    "        # Extract feature names after preprocessing\n",
    "        feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "        pipeline.feature_names = feature_names  # Save feature names to the pipeline\n",
    "\n",
    "        # Save the trained model with the model name and specified suffix\n",
    "        model_path = os.path.join(\n",
    "            paths[\"output_folder\"], \n",
    "            f\"{model_name}{paths['model_name_suffix']}.pkl\"\n",
    "        )\n",
    "        joblib.dump(pipeline, model_path)\n",
    "        print(f\"Trained and saved model: {model_name} to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1db515",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Select models\n",
    "def select_models(config_file):\n",
    "    all_models = {\n",
    "        \"RandomForest\": RandomForestClassifier(**load_model_params(config_file, \"RandomForest\")),\n",
    "        \"AdaBoost\": AdaBoostClassifier(**load_model_params(config_file, \"AdaBoost\")),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(**load_model_params(config_file, \"GradientBoosting\")),\n",
    "        \"KNeighbors\": KNeighborsClassifier(**load_model_params(config_file, \"KNeighbors\")),\n",
    "        \"SVC\": SVC(**load_model_params(config_file, \"SVC\")),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(**load_model_params(config_file, \"DecisionTree\")),\n",
    "        \"LogisticRegression\": LogisticRegression(**load_model_params(config_file, \"LogisticRegression\")),\n",
    "        \"NaiveBayes\": GaussianNB(**load_model_params(config_file, \"NaiveBayes\")),\n",
    "        \"NeuralNetwork\": MLPClassifier(**load_model_params(config_file, \"NeuralNetwork\")),\n",
    "        \"XGBoost\": XGBClassifier(**load_model_params(config_file, \"XGBoost\"))\n",
    "    }\n",
    "    \n",
    "    # Display model selection prompt\n",
    "    print(\"Available models for training:\")\n",
    "    for idx, model_name in enumerate(all_models, start=1):\n",
    "        print(f\"{idx}. {model_name}\")\n",
    "    \n",
    "    selection = input(\"Enter the model numbers to train (comma-separated) or 'all' to train all models: \")\n",
    "    if selection.lower() == 'all':\n",
    "        selected_models = all_models\n",
    "    else:\n",
    "        selected_indices = [int(i.strip()) - 1 for i in selection.split(\",\")]\n",
    "        selected_models = {model_name: model for idx, (model_name, model) in enumerate(all_models.items()) if idx in selected_indices}\n",
    "    \n",
    "    return selected_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7a538",
   "metadata": {},
   "source": [
    "def select_features(data):\n",
    "    print(\"Available features:\")\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        print(f\"{idx + 1}: {column}\")\n",
    "    selected = input(\"Enter the feature numbers to use (comma-separated) or type 'all' to select all: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbad45f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#     if selected.lower() == 'all':\n",
    "#         return data\n",
    "#     else:\n",
    "#         selected_indices = [int(i) - 1 for i in selected.split(',')]\n",
    "#         return data.iloc[:, selected_indices]\n",
    "def select_features(data):\n",
    "    print(\"Available features:\")\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        print(f\"{idx + 1}: {column}\")\n",
    "    selected = input(\"Enter the feature numbers to use (comma-separated), type 'all' to select all, or press Enter for default (2,3,4,5,6): \")\n",
    "    \n",
    "    if selected.lower() == 'all':\n",
    "        return data\n",
    "    elif selected.strip() == '':  # Default option if no input\n",
    "        default_indices = [1, 2, 3, 4, 5]  # 2,3,4,5,6 are indices 1,2,3,4,5 (0-based)\n",
    "        return data.iloc[:, default_indices]\n",
    "    else:\n",
    "        selected_indices = [int(i) - 1 for i in selected.split(',')]\n",
    "        return data.iloc[:, selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfa51a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def normalize_features(data):\n",
    "    print(\"\\nAvailable features for normalization:\")\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        print(f\"{idx + 1}: {column}\")\n",
    "    normalize = input(\"Enter the feature numbers to normalize (comma-separated) or press Enter to skip: \")\n",
    "    \n",
    "    if normalize:\n",
    "        selected_indices = [int(i) - 1 for i in normalize.split(',')]\n",
    "        selected_features = data.columns[selected_indices]\n",
    "        for feature in selected_features:\n",
    "            data[feature] = np.log1p(data[feature]) # Log transformation\n",
    "            print(f\"Applied log transformation on {feature}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606f865",
   "metadata": {},
   "source": [
    "def select_training_file(input_folder):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the cleaned_data folder.\")\n",
    "        return None\n",
    "    print(\"Available files for model training:\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}: {file}\")\n",
    "    choice = int(input(\"Select the file number to train the model on: \")) - 1\n",
    "    return os.path.join(input_folder, files[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6333b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_file(input_folder):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified input folder.\")\n",
    "        return None\n",
    "    print(\"Available files for model training:\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}. {file}\")\n",
    "    choice = int(input(\"Select the file number to use for training: \")) - 1\n",
    "    return os.path.join(input_folder, files[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6fdd6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# def main(input_folder, model_output, config_file=\"config.txt\"):\n",
    "def main(config_file=\"config.txt\"):\n",
    "    if not open_config_file(config_file):\n",
    "        print(\"Exiting due to missing or inaccessible config file.\")\n",
    "        return\n",
    "\n",
    "    # Load paths and model name suffix\n",
    "    paths = load_paths_and_suffix(config_file)\n",
    "\n",
    "    # List files in the input folder and prompt user to select one\n",
    "    data_path = select_training_file(paths[\"input_folder\"])\n",
    "    if data_path is None:\n",
    "        print(\"No valid file selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Select features\n",
    "    data = select_features(data)\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    # target_col = input(\"Enter the target column (label) by name for training (e.g., 'Survived'): \")\n",
    "    target_col = \"Obesity_Level\"\n",
    "    if target_col not in data.columns:\n",
    "        raise ValueError(f\"The specified target column '{target_col}' does not exist in the data.\")\n",
    "\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Select models to train\n",
    "    models = select_models(config_file)\n",
    "\n",
    "    # Normalize features if needed\n",
    "    X = normalize_features(X)\n",
    "    \n",
    "    # Train and save models\n",
    "    train_and_save_models(X, y, paths, models)\n",
    "    print(\"Training completed and models saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b5ba6",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train various models on a selected dataset.\")\n",
    "    parser.add_argument('--input_folder', type=str, default=\"../OTH_DATA/cleaned_data\", help=\"Path to the cleaned data folder.\")\n",
    "    parser.add_argument('--model_output', type=str, default=\"../ML_DATA/model_outputs\", help=\"Path to save the trained models.\")\n",
    "    parser.add_argument('--config_file', type=str, default=\"config.txt\", help=\"Path to the model parameter file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a3cc0",
   "metadata": {},
   "source": [
    "    args = parser.parse_args()\n",
    "    main(args.input_folder, args.model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66002096",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train various models on a selected dataset.\")\n",
    "    parser.add_argument('--config_file', type=str, default=\"config.txt\", help=\"Path to the model parameter file.\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args.config_file)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
