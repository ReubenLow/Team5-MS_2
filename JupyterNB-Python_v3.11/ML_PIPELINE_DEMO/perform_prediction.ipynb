{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a837a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, log_loss, mean_squared_error, confusion_matrix,\n",
    "                             precision_score, recall_score, auc, roc_curve, roc_auc_score,\n",
    "                             f1_score, PrecisionRecallDisplay, RocCurveDisplay,\n",
    "                             ConfusionMatrixDisplay, mean_absolute_error)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "import configparser\n",
    "import joblib\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce1666",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to load folder paths from config\n",
    "def load_paths(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    paths = {\n",
    "        \"input_folder_predict\": config[\"Paths\"].get(\"input_folder_predict\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"model_folder_predict\": config[\"Paths\"].get(\"model_folder_predict\", \"../ML_DATA/model_outputs\"),\n",
    "        \"output_folder_predict\": config[\"Paths\"].get(\"output_folder_predict\", \"../ML_DATA/predict_outputs\")\n",
    "    }\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db447f70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def select_testing_file(input_folder):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the testing_data folder.\")\n",
    "        return None\n",
    "    print(\"Available files for testing:\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}: {file}\")\n",
    "    choice = int(input(\"Select the file number to test the model on: \")) - 1\n",
    "    return os.path.join(input_folder, files[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d8128",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    target_col = \"Obesity_Level\"\n",
    "    X_test = data.drop(columns=[target_col])  \n",
    "    y_test = data[target_col]  \n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909e40",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_models(model_folder):\n",
    "    models = {}\n",
    "    for model_file in os.listdir(model_folder):\n",
    "        if model_file.endswith('.pkl'):\n",
    "            model_path = os.path.join(model_folder, model_file)\n",
    "            model_name = model_file.split('_model.pkl')[0]\n",
    "            models[model_name] = joblib.load(model_path)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38c411",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_test_data(X_test, training_features):\n",
    "    for col in training_features:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0\n",
    "    X_test = X_test[training_features]\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5894c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_predictions_with_formatting(X_test, y_test, y_pred, output_path):\n",
    "    combined_mappings = {\n",
    "        \"MTRANS\": {\n",
    "            0: \"Automobile\",\n",
    "            1: \"Bike\",\n",
    "            2: \"Motorbike\",\n",
    "            3: \"Public_Transportation\",\n",
    "            4: \"Walking\"\n",
    "        },\n",
    "        \"Obesity_Level\": {\n",
    "            0: \"Insufficient_Weight\",\n",
    "            1: \"Normal_Weight\",\n",
    "            2: \"Obesity_Type_I\",\n",
    "            3: \"Obesity_Type_II\",\n",
    "            4: \"Obesity_Type_III\",\n",
    "            5: \"Overweight_Level_I\",\n",
    "            6: \"Overweight_Level_II\"\n",
    "        },\n",
    "        \"Gender\": {0: \"Female\", 1: \"Male\"},\n",
    "        \"fam_hist_over-wt\": {0: \"no\", 1: \"yes\"},\n",
    "        \"FAVC\": {0: \"no\", 1: \"yes\"},\n",
    "        \"CAEC\": {0: \"Always\", 1: \"Frequently\", 2: \"Sometimes\", 3: \"no\"},\n",
    "        \"SMOKE\": {0: \"no\", 1: \"yes\"},\n",
    "        \"SCC\": {0: \"no\", 1: \"yes\"},\n",
    "        \"CALC\": {0: \"Frequently\", 1: \"Sometimes\", 2: \"no\"}\n",
    "    }\n",
    "\n",
    "    # Decode columns in X_test using the combined mappings\n",
    "    for col, mapping in combined_mappings.items():\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].map(mapping)\n",
    "\n",
    "    # Add actual and predicted columns to the dataset\n",
    "    X_test[\"Actual_Obesity_Level\"] = y_test.map(combined_mappings[\"Obesity_Level\"])\n",
    "    X_test[\"Predicted_Obesity_Level\"] = pd.Series(y_pred).map(combined_mappings[\"Obesity_Level\"])\n",
    "\n",
    "    # Create a workbook and worksheet\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Predictions\"\n",
    "\n",
    "    # Add header row\n",
    "    headers = list(X_test.columns)\n",
    "    ws.append(headers)\n",
    "\n",
    "    # Define styles for correct and incorrect predictions\n",
    "    correct_fill = PatternFill(start_color=\"00FF00\", end_color=\"00FF00\", fill_type=\"solid\")  # Green\n",
    "    incorrect_fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")  # Red\n",
    "\n",
    "    # Add data rows with conditional formatting\n",
    "    for i, row in X_test.iterrows():\n",
    "        ws.append(row.tolist())\n",
    "        # Apply formatting to the last column (Predicted_Obesity_Level)\n",
    "        predicted_cell = ws.cell(row=i + 2, column=len(headers))  # Adjust for header row\n",
    "        if row[\"Actual_Obesity_Level\"] == row[\"Predicted_Obesity_Level\"]:\n",
    "            predicted_cell.fill = correct_fill\n",
    "        else:\n",
    "            predicted_cell.fill = incorrect_fill\n",
    "\n",
    "    # Save the Excel file\n",
    "    wb.save(output_path)\n",
    "    print(f\"Predictions with formatting saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240b45d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, output_dir, feature_names):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Call the feature importance plotting function\n",
    "    plot_feature_importances(model, output_dir)\n",
    "\n",
    "    # Generate Excel output with conditional formatting\n",
    "    output_predictions_with_formatting(\n",
    "        X_test.copy(),  # Pass a copy to avoid modifying the original\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        os.path.join(output_dir, \"predictions_with_formatting.xlsx\")\n",
    "    )\n",
    "\n",
    "    # Validate test dataset classes\n",
    "    unique_classes = np.unique(y_test)\n",
    "    print(f\"Unique classes in test set: {unique_classes}\")\n",
    "\n",
    "    # Check if the target is binary or multiclass\n",
    "    is_multiclass = len(unique_classes) > 2\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Create directory for the model if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ROC Curve and AUC for binary or multiclass\n",
    "    if y_prob is not None:\n",
    "        if is_multiclass:\n",
    "            print(\"Handling multiclass ROC curves...\")\n",
    "            y_test_bin = label_binarize(y_test, classes=unique_classes)\n",
    "            n_classes = y_test_bin.shape[1]\n",
    "\n",
    "            # Compute ROC curve and AUC for each class\n",
    "            plt.figure()\n",
    "            for i in range(n_classes):\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, lw=2, label=f'Class {unique_classes[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "            plt.title(\"Multiclass ROC Curve\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.savefig(os.path.join(output_dir, \"multiclass_roc_curve.png\"))\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Plotting binary ROC curve...\")\n",
    "            RocCurveDisplay.from_predictions(y_test, y_prob[:, 1])\n",
    "            plt.title(\"ROC Curve\")\n",
    "            plt.savefig(os.path.join(output_dir, \"roc_curve.png\"))\n",
    "            plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        'roc_auc': None if y_prob is None else (\n",
    "            roc_auc_score(y_test, y_prob, average=\"macro\", multi_class=\"ovr\") if is_multiclass else roc_auc_score(y_test, y_prob[:, 1])\n",
    "        ),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede9b49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, output_dir):\n",
    "    # Access the underlying estimator if model is a pipeline\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        estimator = model.named_steps['model']\n",
    "        feature_names = model.feature_names\n",
    "    else:\n",
    "        estimator = model\n",
    "        feature_names = None\n",
    "\n",
    "    # Check for feature_importances_\n",
    "    if hasattr(estimator, \"feature_importances_\"):\n",
    "        importances = estimator.feature_importances_\n",
    "        print(\"Using feature_importances_ attribute.\")\n",
    "    else:\n",
    "        print(\"Feature importances are not available for this model.\")\n",
    "        return\n",
    "\n",
    "    # Use the stored feature names if available\n",
    "    if feature_names is not None:\n",
    "        feature_importances = pd.DataFrame({\n",
    "            \"Feature\": feature_names,\n",
    "            \"Importance\": importances\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "    else:\n",
    "        feature_importances = pd.DataFrame({\n",
    "            \"Feature\": [f\"Feature {i}\" for i in range(len(importances))],\n",
    "            \"Importance\": importances\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Print feature importances\n",
    "    print(\"Feature Importances:\")\n",
    "    print(feature_importances)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importances[\"Feature\"], feature_importances[\"Importance\"], color=\"skyblue\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.gca().invert_yaxis()  # Reverse the order for readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = os.path.join(output_dir, \"feature_importances.png\")\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Feature importances plot saved to {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3cfbf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to list and select models from the model folder\n",
    "def select_models(model_folder):\n",
    "    model_files = [f for f in os.listdir(model_folder) if f.endswith('.pkl')]\n",
    "    if not model_files:\n",
    "        print(\"No model files found in the specified model folder.\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"Available models for prediction:\")\n",
    "    for idx, model_file in enumerate(model_files, 1):\n",
    "        print(f\"{idx}. {model_file}\")\n",
    "    \n",
    "    selection = input(\"Enter the model numbers to use for prediction (comma-separated) or 'all' to use all models: \")\n",
    "    if selection.lower() == 'all':\n",
    "        selected_files = model_files\n",
    "    else:\n",
    "        selected_indices = [int(i.strip()) - 1 for i in selection.split(\",\")]\n",
    "        selected_files = [model_files[i] for i in selected_indices]\n",
    "    \n",
    "    models = {}\n",
    "    for model_file in selected_files:\n",
    "        model_path = os.path.join(model_folder, model_file)\n",
    "        model_name = model_file.split('_model.pkl')[0]\n",
    "        models[model_name] = joblib.load(model_path)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13742143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_file=\"config.txt\"):\n",
    "    visialize_output_folder = \"../ML_DATA/visualize_output\"\n",
    "    # Load paths from config\n",
    "    paths = load_paths(config_file)\n",
    "\n",
    "    # Select and load test file\n",
    "    test_file = select_testing_file(paths[\"input_folder_predict\"])\n",
    "    if not test_file:\n",
    "        print(\"No valid file selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    X_test, y_test = load_data(test_file)\n",
    "\n",
    "    # Select models to use for prediction\n",
    "    models = select_models(paths[\"model_folder_predict\"])\n",
    "    if not models:\n",
    "        print(\"No models selected for prediction. Exiting.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    report = []\n",
    "    for model_name, model in models.items():\n",
    "        # Extract feature names from the model\n",
    "        try:\n",
    "            feature_names = model.feature_names  # Assuming you saved feature_names in training script\n",
    "        except AttributeError:\n",
    "            print(f\"Could not extract feature names from model {model_name}.\")\n",
    "            continue\n",
    "        model_output_dir = os.path.join(visialize_output_folder, model_name)\n",
    "        # metrics = evaluate_model(model, X_test, y_test, model_output_dir)\n",
    "        metrics = evaluate_model(model, X_test, y_test, model_output_dir, feature_names)\n",
    "        report.append((model_name, metrics))\n",
    "        # print(f\"Model: {model_name} - Metrics: {metrics}\")\n",
    "\n",
    "        # Format and print the metrics\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(metrics['classification_report'])\n",
    "        print(f\"Accuracy       : {metrics['accuracy']:.4f}\")\n",
    "        # if isinstance(metrics['roc_auc'], str):\n",
    "        #     print(f\"ROC AUC        : {metrics['roc_auc']}\")\n",
    "        # else:\n",
    "        #     print(f\"ROC AUC        : {metrics['roc_auc']:.4f}\")\n",
    "        if metrics['roc_auc'] is not None:\n",
    "            print(f\"ROC AUC        : {metrics['roc_auc']:.4f}\")\n",
    "        else:\n",
    "            print(\"ROC AUC        : Not applicable (e.g., no probabilities available).\")\n",
    "        print(f\"MSE            : {metrics['mse']:.4f}\")\n",
    "        print(f\"MAE            : {metrics['mae']:.4f}\")\n",
    "        print(f\"RMSE           : {metrics['rmse']:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(metrics['confusion_matrix'])\n",
    "        print(f\"{'='*40}\\n\")\n",
    "\n",
    "    report_path = os.path.join(visialize_output_folder, 'prediction_report.csv')\n",
    "    report_df = pd.DataFrame([{**{'Model': m}, **metrics} for m, metrics in report])\n",
    "    report_df.to_csv(report_path, index=False)\n",
    "    print(f\"Prediction report with confusion matrices saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # main()\n",
    "    parser = argparse.ArgumentParser(description=\"Run model predictions and evaluations.\")\n",
    "    parser.add_argument('--config_file', type=str, default=\"config.txt\", help=\"Path to the configuration file.\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args.config_file)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
