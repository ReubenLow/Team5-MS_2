{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ddb99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "config_file = \"config.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef05f1",
   "metadata": {
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "import argparse\n",
    "import configparser\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', None)  # Display all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d7de",
   "metadata": {
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_config_file(config_file):\n",
    "    \"\"\"Opens the configuration file for review.\"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuration file '{config_file}' not found.\")\n",
    "        return False\n",
    "    print(f\"Using configuration file: {config_file}\")\n",
    "    return True\n",
    "\n",
    "def load_paths_and_suffix(config_file):\n",
    "    \"\"\"Returns hardcoded paths and file suffixes for the workflow.\"\"\"\n",
    "    # return {\n",
    "    #     \"input_folder\": \"../OTH_DATA/training_data\",\n",
    "    #     \"output_folder\": \"../OTH_DATA/cleaned_data\",\n",
    "    #     \"cleaned_file_suffix\": \"_TESTCASE2\"\n",
    "    # }\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    paths = {\n",
    "        \"input_folder\": config[\"Paths\"].get(\"input_folder_clean\", \"../OTH_DATA/training_data\"),\n",
    "        \"output_folder\": config[\"Paths\"].get(\"output_folder_clean\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"cleaned_file_suffix\": config[\"Paths\"].get(\"cleaned_file_suffix\", \"_v1\")\n",
    "    }\n",
    "    return paths\n",
    "\n",
    "def calculate_body_fat(data):\n",
    "    # Ensure BMI and Age exist in the dataset\n",
    "    if 'BMI' not in data.columns or 'Age' not in data.columns:\n",
    "        raise ValueError(\"The dataset must contain 'BMI' and 'Age' columns to calculate Body Fat.\")\n",
    "\n",
    "    # Check for missing values in required columns\n",
    "    if data[['BMI', 'Age', 'Gender']].isnull().any().any():\n",
    "        print(\"Warning: Missing values detected in 'BMI', 'Age', or 'Gender'. These rows will have NaN for Body Fat.\")\n",
    "\n",
    "    # Calculate Body Fat for Males (Gender == 1)\n",
    "    data.loc[data['Gender'] == 1, 'Body_Fat'] = (\n",
    "        1.20 * data['BMI'] + 0.23 * data['Age'] - 16.2\n",
    "    )\n",
    "    # Calculate Body Fat for Females (Gender == 0)\n",
    "    data.loc[data['Gender'] == 0, 'Body_Fat'] = (\n",
    "        1.20 * data['BMI'] + 0.23 * data['Age'] - 5.4\n",
    "    )\n",
    "    data['Body_Fat'] = data['Body_Fat'].round(2)\n",
    "    return data\n",
    "\n",
    "# def clean_data(data, drop_columns=None, add_target=False, target_column_name=\"target\"):\n",
    "#     \"\"\"Cleans the data by dropping columns, adding BMI, and encoding categorical data.\"\"\"\n",
    "#     # If there is extra trailing delimiter, pandas will create an extra column 'Unnamed: 18'\n",
    "#     if 'Unnamed: 18' in data.columns:\n",
    "#         data = data.drop(columns=['Unnamed: 18'])\n",
    "\n",
    "#     if drop_columns:\n",
    "#         data = data.drop(columns=drop_columns, errors=\"ignore\")\n",
    "#     if \"Height\" in data.columns and \"Weight\" in data.columns:\n",
    "#         data[\"BMI\"] = data[\"Weight\"] / (data[\"Height\"] ** 2)\n",
    "#         data['BMI'] = data['BMI'].round(2)\n",
    "#     if add_target and target_column_name not in data.columns:\n",
    "#         data[target_column_name] = None\n",
    "#     categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "#     for col in categorical_cols:\n",
    "#         data[col] = data[col].astype(\"category\").cat.codes\n",
    "#     return data\n",
    "\n",
    "def clean_data(data, drop_columns=None, add_target=False, target_column_name=\"target\"):\n",
    "    # Drop unnecessary columns\n",
    "    # If there is extra trailing delimiter, pandas will create an extra column 'Unnamed: 18'\n",
    "    if 'Unnamed: 18' in data.columns:\n",
    "        data = data.drop(columns=['Unnamed: 18'])\n",
    "\n",
    "    # Drop user-specified columns\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns, errors='ignore')\n",
    "        print(f\"Dropped columns: {drop_columns}\")\n",
    "\n",
    "    # Handle missing values, fill numerical columns with median\n",
    "    for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            data[column].fillna(data[column].median(), inplace=True)\n",
    "\n",
    "    # Calculate BMI if Height and Weight columns are present\n",
    "    if 'Height' in data.columns and 'Weight' in data.columns:\n",
    "       data['BMI'] = data['Weight'] / ((data['Height']) ** 2)\n",
    "       data['BMI'] = data['BMI'].round(2)\n",
    "\n",
    "    # Encoding categorical variables with numbers\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    for column in categorical_columns:\n",
    "        data[column] = data[column].astype('category').cat.codes\n",
    "\n",
    "    # Add an empty target column if requested for dataset with no target column\n",
    "    if add_target and target_column_name not in data.columns:\n",
    "        data[target_column_name] = None\n",
    "        print(f\"Added an empty target column: '{target_column_name}'\")\n",
    "\n",
    "    data = calculate_body_fat(data)\n",
    "    print(\"Data cleaning complete.\")\n",
    "    return data\n",
    "\n",
    "def generate_encoding_summary(original_data, encoded_data):\n",
    "    \"\"\"Generates a summary of the encoding performed.\"\"\"\n",
    "    summary = {}\n",
    "    categorical_columns = original_data.select_dtypes(include=[\"object\"]).columns\n",
    "    for column in categorical_columns:\n",
    "        if column in encoded_data.columns:\n",
    "            original_col = original_data[column].astype(\"category\")\n",
    "            summary[column] = dict(enumerate(original_col.cat.categories))\n",
    "    return summary\n",
    "\n",
    "def print_encoding_summary(encoding_summary):\n",
    "    \"\"\"Prints the encoding summary.\"\"\"\n",
    "    for column, mapping in encoding_summary.items():\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        for code, category in mapping.items():\n",
    "            print(f\"  {code} -> {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f31094",
   "metadata": {
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Open configuration file and confirm paths\n",
    "config_file = \"../SCRIPTS_CFG/config.txt\"\n",
    "\n",
    "if not open_config_file(config_file):\n",
    "    print(\"Exiting due to missing or inaccessible config file.\")\n",
    "else:\n",
    "    # Load paths from the configuration file\n",
    "    paths = load_paths_and_suffix(config_file)\n",
    "    input_folder = paths[\"input_folder\"]\n",
    "    output_folder = paths[\"output_folder\"]\n",
    "    cleaned_file_suffix = paths[\"cleaned_file_suffix\"]\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Proceed with the rest of the workflow (listing files, cleaning data, etc.)\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the input folder.\")\n",
    "    else:\n",
    "        print(\"\\nAvailable files:\")\n",
    "        for i, file in enumerate(files, 1):\n",
    "            print(f\"{i}. {file}\")\n",
    "\n",
    "        # User selects a file to clean\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number of the file you want to clean: \"))\n",
    "            selected_file = files[choice - 1]\n",
    "            print(f\"Selected file: {selected_file}\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid selection.\")\n",
    "            selected_file = None\n",
    "\n",
    "        if selected_file:\n",
    "            # Rest of the workflow for cleaning data\n",
    "            input_path = os.path.join(input_folder, selected_file)\n",
    "            data = pd.read_csv(input_path)\n",
    "\n",
    "            # Ask for additional columns to drop\n",
    "            user_input = input(\"Enter additional columns to drop (comma-separated) or press Enter to skip: \")\n",
    "            drop_columns = [\"Patient ID\"]\n",
    "            if user_input:\n",
    "                drop_columns += [col.strip() for col in user_input.split(\",\")]\n",
    "\n",
    "            add_target = input(\"Add an empty target column? (yes/no): \").strip().lower() == \"yes\"\n",
    "            target_column_name = \"target\" if not add_target else input(\"Enter target column name: \"\n",
    "                                                                      )\n",
    "\n",
    "            # Clean the data\n",
    "            cleaned_data = clean_data(data, drop_columns=drop_columns, add_target=add_target, target_column_name=target_column_name)\n",
    "\n",
    "            # Save the cleaned data\n",
    "            output_filename = f\"cleaned_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cleaned_data.to_csv(output_path, index=False)\n",
    "            print(f\"Cleaned data saved to: {output_path}\")\n",
    "\n",
    "            # Reload the cleaned data and split if required\n",
    "            cleaned_data = pd.read_csv(output_path)\n",
    "\n",
    "            # Ask the user if they want to split the data\n",
    "            split_data = input(\"Do you want to split the data into training and testing sets? (yes/no): \").strip().lower() == \"yes\"\n",
    "            if split_data:\n",
    "                try:\n",
    "                    # Prompt user for split percentages\n",
    "                    train_percentage = float(input(\"Enter the percentage for training data (e.g., 80 for 80%): \")) / 100\n",
    "                    validation_percentage = float(input(\"Enter the percentage for validation data (e.g., 10 for 10%): \")) / 100\n",
    "                    test_percentage = float(input(\"Enter the percentage for test data (e.g., 10 for 10%): \")) / 100\n",
    "\n",
    "                    # Ensure the percentages add up to 1\n",
    "                    if not abs(train_percentage + validation_percentage + test_percentage - 1) < 1e-5:\n",
    "                        raise ValueError(\"Percentages must add up to 100%!\")\n",
    "                except ValueError as e:\n",
    "                    print(f\"Invalid input: {e}\")\n",
    "                    print(\"Using default split: 70% train, 15% validation, 15% test.\")\n",
    "                    train_percentage, validation_percentage, test_percentage = 0.7, 0.15, 0.15\n",
    "\n",
    "                # Perform the data splitting\n",
    "                if validation_percentage == 0:\n",
    "                    # Split into training and testing only\n",
    "                    train_data, test_data = train_test_split(cleaned_data, test_size=test_percentage, random_state=42)\n",
    "                    validation_data = None\n",
    "                else:\n",
    "                    # Split into training, validation, and testing\n",
    "                    train_data, temp_data = train_test_split(cleaned_data, test_size=(validation_percentage + test_percentage), random_state=42)\n",
    "                    validation_data, test_data = train_test_split(temp_data, test_size=(test_percentage / (validation_percentage + test_percentage)), random_state=42)\n",
    "\n",
    "                # Save the split datasets\n",
    "                train_output_path = os.path.join(output_folder, f\"train_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                test_output_path = os.path.join(output_folder, f\"test_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                train_data.to_csv(train_output_path, index=False)\n",
    "                test_data.to_csv(test_output_path, index=False)\n",
    "                print(f\"Saved training data to {train_output_path}\")\n",
    "                print(f\"Saved testing data to {test_output_path}\")\n",
    "\n",
    "                # Save validation data if it exists\n",
    "                if validation_data is not None:\n",
    "                    validation_output_path = os.path.join(output_folder, f\"validation_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                    validation_data.to_csv(validation_output_path, index=False)\n",
    "                    print(f\"Saved validation data to {validation_output_path}\")\n",
    "\n",
    "            # Generate and print encoding summary\n",
    "            encoding_summary = generate_encoding_summary(data, cleaned_data)\n",
    "            print_encoding_summary(encoding_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224bfd8-8c59-453e-b2f7-9e0f5d8fc79f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.614,
   "end_time": "2024-11-20T08:20:51.882436",
   "environment_variables": {},
   "exception": null,
   "input_path": "clean_data.ipynb",
   "output_path": "clean_data_output.ipynb",
   "parameters": {
    "config_file": "config.txt"
   },
   "start_time": "2024-11-20T08:20:51.268436",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}