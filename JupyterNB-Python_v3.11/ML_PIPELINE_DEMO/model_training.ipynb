{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d9820bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import configparser\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9b056e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# def open_config_file(config_file):\n",
    "#     \"\"\"Opens the configuration file for user review.\"\"\"\n",
    "#     # Resolve the absolute path of the configuration file\n",
    "#     absolute_path = os.path.abspath(config_file)\n",
    "\n",
    "#     if not os.path.exists(absolute_path):\n",
    "#         print(f\"Configuration file '{absolute_path}' not found. Please ensure the file exists.\")\n",
    "#         return False\n",
    "\n",
    "#     try:\n",
    "#         # Open the file in the default text editor\n",
    "#         if os.name == 'nt':  # Windows\n",
    "#             os.startfile(absolute_path)\n",
    "#         else:\n",
    "#             subprocess.Popen(['open' if os.name == 'posix' else 'xdg-open', absolute_path])\n",
    "        \n",
    "#         print(f\"Opened configuration file: {absolute_path}\")\n",
    "#         input(\"Press Enter once you've reviewed and saved the configuration file...\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to open the configuration file: {e}\")\n",
    "#         return False\n",
    "\n",
    "def open_config_file(config_file):\n",
    "    # Check if the config file exists\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuration file '{config_file}' not found. Please make sure it exists.\")\n",
    "        return False\n",
    "    \n",
    "    # Open the config file in the default text editor\n",
    "    try:\n",
    "        print(f\"Opening configuration file '{config_file}' for review...\")\n",
    "        subprocess.Popen(['open' if os.name == 'posix' else 'start', config_file], shell=True)\n",
    "        input(\"Press Enter when you're ready to proceed with model training...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open the configuration file: {e}\")\n",
    "        return False\n",
    "\n",
    "# def open_config_file(config_file):\n",
    "#     \"\"\"Opens the configuration file for review.\"\"\"\n",
    "#     if not os.path.exists(config_file):\n",
    "#         print(f\"Configuration file '{config_file}' not found.\")\n",
    "#         return False\n",
    "#     print(f\"Using configuration file: {config_file}\")\n",
    "#     return True\n",
    "\n",
    "def load_model_params(config_file, model_name):\n",
    "    \"\"\"Loads model parameters from the configuration file.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    params = {}\n",
    "    for key, value in config[model_name].items():\n",
    "        try:\n",
    "            params[key] = eval(value)  # Evaluate values for literals\n",
    "        except (NameError, SyntaxError):\n",
    "            params[key] = value.strip(\"'\\\"\")\n",
    "    return params\n",
    "\n",
    "def load_paths_and_suffix(config_file):\n",
    "    \"\"\"Loads input/output paths and suffix from the configuration file.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    return {\n",
    "        \"input_folder\": config[\"Paths\"].get(\"input_folder\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"output_folder\": config[\"Paths\"].get(\"output_folder\", \"../ML_DATA/model_outputs\"),\n",
    "        \"model_name_suffix\": config[\"Paths\"].get(\"model_name_suffix\", \"_v1\"),\n",
    "    }\n",
    "\n",
    "def select_training_file(input_folder):\n",
    "    \"\"\"Prompts user to select a training file from the input folder.\"\"\"\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return None\n",
    "    print(\"Available files for training:\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}. {file}\")\n",
    "    try:\n",
    "        choice = int(input(\"Select the file number to use for training: \")) - 1\n",
    "        return os.path.join(input_folder, files[choice])\n",
    "    except (ValueError, IndexError):\n",
    "        print(\"Invalid selection.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8bb9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_models(config_file):\n",
    "    all_models = {\n",
    "        \"RandomForest\": RandomForestClassifier(**load_model_params(config_file, \"RandomForest\")),\n",
    "        \"AdaBoost\": AdaBoostClassifier(**load_model_params(config_file, \"AdaBoost\")),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(**load_model_params(config_file, \"GradientBoosting\")),\n",
    "        \"KNeighbors\": KNeighborsClassifier(**load_model_params(config_file, \"KNeighbors\")),\n",
    "        \"SVC\": SVC(**load_model_params(config_file, \"SVC\")),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(**load_model_params(config_file, \"DecisionTree\")),\n",
    "        \"LogisticRegression\": LogisticRegression(**load_model_params(config_file, \"LogisticRegression\")),\n",
    "        \"NaiveBayes\": GaussianNB(**load_model_params(config_file, \"NaiveBayes\")),\n",
    "        \"NeuralNetwork\": MLPClassifier(**load_model_params(config_file, \"NeuralNetwork\")),\n",
    "        \"XGBoost\": XGBClassifier(**load_model_params(config_file, \"XGBoost\"))\n",
    "    }\n",
    "    \n",
    "    # Display model selection prompt\n",
    "    print(\"Available models for training:\")\n",
    "    for idx, model_name in enumerate(all_models, start=1):\n",
    "        print(f\"{idx}. {model_name}\")\n",
    "    \n",
    "    selection = input(\"Enter the model numbers to train (comma-separated) or 'all' to train all models: \")\n",
    "    if selection.lower() == 'all':\n",
    "        selected_models = all_models\n",
    "    else:\n",
    "        selected_indices = [int(i.strip()) - 1 for i in selection.split(\",\")]\n",
    "        selected_models = {model_name: model for idx, (model_name, model) in enumerate(all_models.items()) if idx in selected_indices}\n",
    "    \n",
    "    return selected_models\n",
    "\n",
    "# def train_and_save_models(X, y, paths, models):\n",
    "#     \"\"\"Trains and saves selected models.\"\"\"\n",
    "#     os.makedirs(paths[\"output_folder\"], exist_ok=True)\n",
    "#     categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "#     numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "#     preprocessor = ColumnTransformer([\n",
    "#         ('num', StandardScaler(), numeric_cols),\n",
    "#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "#     ])\n",
    "#     for model_name, model in models.items():\n",
    "#         pipeline = Pipeline([\n",
    "#             ('preprocessor', preprocessor),\n",
    "#             ('model', model),\n",
    "#         ])\n",
    "#         pipeline.fit(X, y)\n",
    "#         model_path = os.path.join(paths[\"output_folder\"], f\"{model_name}{paths['model_name_suffix']}.pkl\")\n",
    "#         joblib.dump(pipeline, model_path)\n",
    "#         print(f\"Saved model: {model_name} to {model_path}\")\n",
    "\n",
    "def train_and_save_models(X, y, paths, models):\n",
    "    if not os.path.exists(paths[\"output_folder\"]):\n",
    "        os.makedirs(paths[\"output_folder\"])\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X, y)\n",
    "\n",
    "        # Extract feature names after preprocessing\n",
    "        feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "        pipeline.feature_names = feature_names  # Save feature names to the pipeline\n",
    "\n",
    "        # Save the trained model with the model name and specified suffix\n",
    "        model_path = os.path.join(\n",
    "            paths[\"output_folder\"], \n",
    "            f\"{model_name}{paths['model_name_suffix']}.pkl\"\n",
    "        )\n",
    "        joblib.dump(pipeline, model_path)\n",
    "        print(f\"Trained and saved model: {model_name} to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "197bbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(data):\n",
    "    print(\"Available features:\")\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        print(f\"{idx + 1}: {column}\")\n",
    "    selected = input(\"Enter the feature numbers to use (comma-separated), type 'all' to select all, or press Enter for default (2,3,4,5,6): \")\n",
    "    \n",
    "    if selected.lower() == 'all':\n",
    "        return data\n",
    "    elif selected.strip() == '':  # Default option if no input\n",
    "        default_indices = [1, 2, 3, 4, 5]  # 2,3,4,5,6 are indices 1,2,3,4,5 (0-based)\n",
    "        return data.iloc[:, default_indices]\n",
    "    else:\n",
    "        selected_indices = [int(i) - 1 for i in selected.split(',')]\n",
    "        return data.iloc[:, selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a11d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(data):\n",
    "    print(\"\\nAvailable features for normalization:\")\n",
    "    for idx, column in enumerate(data.columns):\n",
    "        print(f\"{idx + 1}: {column}\")\n",
    "    normalize = input(\"Enter the feature numbers to normalize (comma-separated) or press Enter to skip: \")\n",
    "    \n",
    "    if normalize:\n",
    "        selected_indices = [int(i) - 1 for i in normalize.split(',')]\n",
    "        selected_features = data.columns[selected_indices]\n",
    "        for feature in selected_features:\n",
    "            data[feature] = np.log1p(data[feature]) # Log transformation\n",
    "            print(f\"Applied log transformation on {feature}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe257d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_file(input_folder):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the specified input folder.\")\n",
    "        return None\n",
    "    print(\"Available files for model training:\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}. {file}\")\n",
    "    choice = int(input(\"Select the file number to use for training: \")) - 1\n",
    "    return os.path.join(input_folder, files[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f517237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening configuration file '../SCRIPTS_CFG/config.txt' for review...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter when you're ready to proceed with model training... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files for model training:\n",
      "1. cleaned_MS_2_Scenario_data_TESTCASE2.csv\n",
      "2. cleaned_MS_2_Scenario_data_TESTCASE3.csv\n",
      "3. cleaned_MS_2_Scenario_data_testSplit.csv\n",
      "4. cleaned_MS_2_Scenario_data_v1.csv\n",
      "5. test_MS_2_Scenario_data_TESTCASE2.csv\n",
      "6. test_MS_2_Scenario_data_TESTCASE3.csv\n",
      "7. test_MS_2_Scenario_data_testSplit.csv\n",
      "8. test_MS_2_Scenario_data_v1.csv\n",
      "9. train_MS_2_Scenario_data_TESTCASE2.csv\n",
      "10. train_MS_2_Scenario_data_TESTCASE3.csv\n",
      "11. train_MS_2_Scenario_data_testSplit.csv\n",
      "12. train_MS_2_Scenario_data_v1.csv\n",
      "13. validation_MS_2_Scenario_data_TESTCASE2.csv\n",
      "14. validation_MS_2_Scenario_data_TESTCASE3.csv\n",
      "15. validation_MS_2_Scenario_data_testSplit.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select the file number to use for training:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available features:\n",
      "1: Gender\n",
      "2: Age\n",
      "3: Height\n",
      "4: Weight\n",
      "5: fam_hist_over-wt\n",
      "6: FAVC\n",
      "7: FCVC\n",
      "8: NCP\n",
      "9: CAEC\n",
      "10: SMOKE\n",
      "11: CH2O\n",
      "12: SCC\n",
      "13: FAF\n",
      "14: TUE\n",
      "15: CALC\n",
      "16: MTRANS\n",
      "17: Obesity_Level\n",
      "18: BMI\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the feature numbers to use (comma-separated), type 'all' to select all, or press Enter for default (2,3,4,5,6):  all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models for training:\n",
      "1. RandomForest\n",
      "2. AdaBoost\n",
      "3. GradientBoosting\n",
      "4. KNeighbors\n",
      "5. SVC\n",
      "6. DecisionTree\n",
      "7. LogisticRegression\n",
      "8. NaiveBayes\n",
      "9. NeuralNetwork\n",
      "10. XGBoost\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the model numbers to train (comma-separated) or 'all' to train all models:  all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available features for normalization:\n",
      "1: Gender\n",
      "2: Age\n",
      "3: Height\n",
      "4: Weight\n",
      "5: fam_hist_over-wt\n",
      "6: FAVC\n",
      "7: FCVC\n",
      "8: NCP\n",
      "9: CAEC\n",
      "10: SMOKE\n",
      "11: CH2O\n",
      "12: SCC\n",
      "13: FAF\n",
      "14: TUE\n",
      "15: CALC\n",
      "16: MTRANS\n",
      "17: BMI\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the feature numbers to normalize (comma-separated) or press Enter to skip:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained and saved model: RandomForest to ../ML_DATA/model_outputs\\RandomForest_demo.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained and saved model: AdaBoost to ../ML_DATA/model_outputs\\AdaBoost_demo.pkl\n",
      "Trained and saved model: GradientBoosting to ../ML_DATA/model_outputs\\GradientBoosting_demo.pkl\n",
      "Trained and saved model: KNeighbors to ../ML_DATA/model_outputs\\KNeighbors_demo.pkl\n",
      "Trained and saved model: SVC to ../ML_DATA/model_outputs\\SVC_demo.pkl\n",
      "Trained and saved model: DecisionTree to ../ML_DATA/model_outputs\\DecisionTree_demo.pkl\n",
      "Trained and saved model: LogisticRegression to ../ML_DATA/model_outputs\\LogisticRegression_demo.pkl\n",
      "Trained and saved model: NaiveBayes to ../ML_DATA/model_outputs\\NaiveBayes_demo.pkl\n",
      "Trained and saved model: NeuralNetwork to ../ML_DATA/model_outputs\\NeuralNetwork_demo.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:49:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained and saved model: XGBoost to ../ML_DATA/model_outputs\\XGBoost_demo.pkl\n",
      "Training completed and models saved.\n"
     ]
    }
   ],
   "source": [
    "config_file = \"../SCRIPTS_CFG/config.txt\"\n",
    "if not open_config_file(config_file):\n",
    "    print(\"Exiting due to missing or inaccessible config file.\")\n",
    "\n",
    "else:\n",
    "    # Load paths and model name suffix\n",
    "    paths = load_paths_and_suffix(config_file)\n",
    "\n",
    "    # List files in the input folder and prompt user to select one\n",
    "    data_path = select_training_file(paths[\"input_folder\"])\n",
    "    if data_path is None:\n",
    "        print(\"No valid file selected. Exiting.\")\n",
    "\n",
    "    else:\n",
    "        data = pd.read_csv(data_path)\n",
    "\n",
    "        # Select features\n",
    "        data = select_features(data)\n",
    "\n",
    "        # Separate features and target variable\n",
    "        # target_col = input(\"Enter the target column (label) by name for training (e.g., 'Survived'): \")\n",
    "        target_col = \"Obesity_Level\"\n",
    "        if target_col not in data.columns:\n",
    "            raise ValueError(f\"The specified target column '{target_col}' does not exist in the data.\")\n",
    "\n",
    "        X = data.drop(columns=[target_col])\n",
    "        y = data[target_col]\n",
    "\n",
    "        # Select models to train\n",
    "        models = select_models(config_file)\n",
    "\n",
    "        # Normalize features if needed\n",
    "        X = normalize_features(X)\n",
    "\n",
    "        # Train and save models\n",
    "        train_and_save_models(X, y, paths, models)\n",
    "        print(\"Training completed and models saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610d2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
