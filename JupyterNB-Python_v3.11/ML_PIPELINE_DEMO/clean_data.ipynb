{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef05f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import shapiro, kstest, norm, probplot, chi2_contingency\n",
    "import argparse\n",
    "import configparser\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', None)  # Display all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d7de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def open_config_file(config_file):\n",
    "    \"\"\"Opens the configuration file for review.\"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Configuration file '{config_file}' not found.\")\n",
    "        return False\n",
    "    print(f\"Using configuration file: {config_file}\")\n",
    "    return True\n",
    "\n",
    "def load_paths_and_suffix(config_file):\n",
    "    \"\"\"Returns hardcoded paths and file suffixes for the workflow.\"\"\"\n",
    "    # return {\n",
    "    #     \"input_folder\": \"../OTH_DATA/training_data\",\n",
    "    #     \"output_folder\": \"../OTH_DATA/cleaned_data\",\n",
    "    #     \"cleaned_file_suffix\": \"_TESTCASE2\"\n",
    "    # }\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    paths = {\n",
    "        \"input_folder\": config[\"Paths\"].get(\"input_folder_clean\", \"../OTH_DATA/training_data\"),\n",
    "        \"output_folder\": config[\"Paths\"].get(\"output_folder_clean\", \"../OTH_DATA/cleaned_data\"),\n",
    "        \"cleaned_file_suffix\": config[\"Paths\"].get(\"cleaned_file_suffix\", \"_v1\")\n",
    "    }\n",
    "    return paths\n",
    "\n",
    "def clean_data(data, drop_columns=None, add_target=False, target_column_name=\"target\"):\n",
    "    \"\"\"Cleans the data by dropping columns, adding BMI, and encoding categorical data.\"\"\"\n",
    "    if drop_columns:\n",
    "        data = data.drop(columns=drop_columns, errors=\"ignore\")\n",
    "    if \"Height\" in data.columns and \"Weight\" in data.columns:\n",
    "        data[\"BMI\"] = data[\"Weight\"] / (data[\"Height\"] ** 2)\n",
    "        data['BMI'] = data['BMI'].round(2)\n",
    "    if add_target and target_column_name not in data.columns:\n",
    "        data[target_column_name] = None\n",
    "    categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].astype(\"category\").cat.codes\n",
    "    return data\n",
    "\n",
    "def generate_encoding_summary(original_data, encoded_data):\n",
    "    \"\"\"Generates a summary of the encoding performed.\"\"\"\n",
    "    summary = {}\n",
    "    categorical_columns = original_data.select_dtypes(include=[\"object\"]).columns\n",
    "    for column in categorical_columns:\n",
    "        if column in encoded_data.columns:\n",
    "            original_col = original_data[column].astype(\"category\")\n",
    "            summary[column] = dict(enumerate(original_col.cat.categories))\n",
    "    return summary\n",
    "\n",
    "def print_encoding_summary(encoding_summary):\n",
    "    \"\"\"Prints the encoding summary.\"\"\"\n",
    "    for column, mapping in encoding_summary.items():\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        for code, category in mapping.items():\n",
    "            print(f\"  {code} -> {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f31094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file: ../SCRIPTS_CFG/config.txt\n",
      "\n",
      "Available files:\n",
      "1. MS_2_Scenario_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of the file you want to clean:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected file: MS_2_Scenario_data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter additional columns to drop (comma-separated) or press Enter to skip:  \n",
      "Add an empty target column? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ../OTH_DATA/cleaned_data\\cleaned_MS_2_Scenario_data_TESTCASE3.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to split the data into training and testing sets? (yes/no):  yes\n",
      "Enter the percentage for training data (e.g., 80 for 80%):  80\n",
      "Enter the percentage for validation data (e.g., 10 for 10%):  0\n",
      "Enter the percentage for test data (e.g., 10 for 10%):  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training data to ../OTH_DATA/cleaned_data\\train_MS_2_Scenario_data_TESTCASE3.csv\n",
      "Saved testing data to ../OTH_DATA/cleaned_data\\test_MS_2_Scenario_data_TESTCASE3.csv\n",
      "\n",
      "Column: Gender\n",
      "  0 -> Female\n",
      "  1 -> Male\n",
      "\n",
      "Column: fam_hist_over-wt\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: FAVC\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: CAEC\n",
      "  0 -> Always\n",
      "  1 -> Frequently\n",
      "  2 -> Sometimes\n",
      "  3 -> no\n",
      "\n",
      "Column: SMOKE\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: SCC\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: CALC\n",
      "  0 -> Frequently\n",
      "  1 -> Sometimes\n",
      "  2 -> no\n",
      "\n",
      "Column: MTRANS\n",
      "  0 -> Automobile\n",
      "  1 -> Bike\n",
      "  2 -> Motorbike\n",
      "  3 -> Public_Transportation\n",
      "  4 -> Walking\n",
      "\n",
      "Column: Obesity_Level\n",
      "  0 -> Insufficient_Weight\n",
      "  1 -> Normal_Weight\n",
      "  2 -> Obesity_Type_I\n",
      "  3 -> Obesity_Type_II\n",
      "  4 -> Obesity_Type_III\n",
      "  5 -> Overweight_Level_I\n",
      "  6 -> Overweight_Level_II\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter additional columns to drop (comma-separated) or press Enter to skip:  \n",
      "Add an empty target column? (yes/no):  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ../OTH_DATA/cleaned_data\\cleaned_MS_2_Scenario_data_TESTCASE2.csv\n",
      "\n",
      "Column: Gender\n",
      "  0 -> Female\n",
      "  1 -> Male\n",
      "\n",
      "Column: fam_hist_over-wt\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: FAVC\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: CAEC\n",
      "  0 -> Always\n",
      "  1 -> Frequently\n",
      "  2 -> Sometimes\n",
      "  3 -> no\n",
      "\n",
      "Column: SMOKE\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: SCC\n",
      "  0 -> no\n",
      "  1 -> yes\n",
      "\n",
      "Column: CALC\n",
      "  0 -> Frequently\n",
      "  1 -> Sometimes\n",
      "  2 -> no\n",
      "\n",
      "Column: MTRANS\n",
      "  0 -> Automobile\n",
      "  1 -> Bike\n",
      "  2 -> Motorbike\n",
      "  3 -> Public_Transportation\n",
      "  4 -> Walking\n",
      "\n",
      "Column: Obesity_Level\n",
      "  0 -> Insufficient_Weight\n",
      "  1 -> Normal_Weight\n",
      "  2 -> Obesity_Type_I\n",
      "  3 -> Obesity_Type_II\n",
      "  4 -> Obesity_Type_III\n",
      "  5 -> Overweight_Level_I\n",
      "  6 -> Overweight_Level_II\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Open configuration file and confirm paths\n",
    "config_file = \"../SCRIPTS_CFG/config.txt\"\n",
    "\n",
    "if not open_config_file(config_file):\n",
    "    print(\"Exiting due to missing or inaccessible config file.\")\n",
    "else:\n",
    "    # Load paths from the configuration file\n",
    "    paths = load_paths_and_suffix(config_file)\n",
    "    input_folder = paths[\"input_folder\"]\n",
    "    output_folder = paths[\"output_folder\"]\n",
    "    cleaned_file_suffix = paths[\"cleaned_file_suffix\"]\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Proceed with the rest of the workflow (listing files, cleaning data, etc.)\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the input folder.\")\n",
    "    else:\n",
    "        print(\"\\nAvailable files:\")\n",
    "        for i, file in enumerate(files, 1):\n",
    "            print(f\"{i}. {file}\")\n",
    "\n",
    "        # User selects a file to clean\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number of the file you want to clean: \"))\n",
    "            selected_file = files[choice - 1]\n",
    "            print(f\"Selected file: {selected_file}\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid selection.\")\n",
    "            selected_file = None\n",
    "\n",
    "        if selected_file:\n",
    "            # Rest of the workflow for cleaning data\n",
    "            input_path = os.path.join(input_folder, selected_file)\n",
    "            data = pd.read_csv(input_path)\n",
    "\n",
    "            # Ask for additional columns to drop\n",
    "            user_input = input(\"Enter additional columns to drop (comma-separated) or press Enter to skip: \")\n",
    "            drop_columns = [\"Patient ID\"]\n",
    "            if user_input:\n",
    "                drop_columns += [col.strip() for col in user_input.split(\",\")]\n",
    "\n",
    "            add_target = input(\"Add an empty target column? (yes/no): \").strip().lower() == \"yes\"\n",
    "            target_column_name = \"target\" if not add_target else input(\"Enter target column name: \"\n",
    "                                                                      )\n",
    "\n",
    "            # Clean the data\n",
    "            cleaned_data = clean_data(data, drop_columns=drop_columns, add_target=add_target, target_column_name=target_column_name)\n",
    "\n",
    "            # Save the cleaned data\n",
    "            output_filename = f\"cleaned_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cleaned_data.to_csv(output_path, index=False)\n",
    "            print(f\"Cleaned data saved to: {output_path}\")\n",
    "\n",
    "            # Reload the cleaned data and split if required\n",
    "            cleaned_data = pd.read_csv(output_path)\n",
    "\n",
    "            # Ask the user if they want to split the data\n",
    "            split_data = input(\"Do you want to split the data into training and testing sets? (yes/no): \").strip().lower() == \"yes\"\n",
    "            if split_data:\n",
    "                try:\n",
    "                    # Prompt user for split percentages\n",
    "                    train_percentage = float(input(\"Enter the percentage for training data (e.g., 80 for 80%): \")) / 100\n",
    "                    validation_percentage = float(input(\"Enter the percentage for validation data (e.g., 10 for 10%): \")) / 100\n",
    "                    test_percentage = float(input(\"Enter the percentage for test data (e.g., 10 for 10%): \")) / 100\n",
    "\n",
    "                    # Ensure the percentages add up to 1\n",
    "                    if not abs(train_percentage + validation_percentage + test_percentage - 1) < 1e-5:\n",
    "                        raise ValueError(\"Percentages must add up to 100%!\")\n",
    "                except ValueError as e:\n",
    "                    print(f\"Invalid input: {e}\")\n",
    "                    print(\"Using default split: 70% train, 15% validation, 15% test.\")\n",
    "                    train_percentage, validation_percentage, test_percentage = 0.7, 0.15, 0.15\n",
    "\n",
    "                # Perform the data splitting\n",
    "                if validation_percentage == 0:\n",
    "                    # Split into training and testing only\n",
    "                    train_data, test_data = train_test_split(cleaned_data, test_size=test_percentage, random_state=42)\n",
    "                    validation_data = None\n",
    "                else:\n",
    "                    # Split into training, validation, and testing\n",
    "                    train_data, temp_data = train_test_split(cleaned_data, test_size=(validation_percentage + test_percentage), random_state=42)\n",
    "                    validation_data, test_data = train_test_split(temp_data, test_size=(test_percentage / (validation_percentage + test_percentage)), random_state=42)\n",
    "\n",
    "                # Save the split datasets\n",
    "                train_output_path = os.path.join(output_folder, f\"train_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                test_output_path = os.path.join(output_folder, f\"test_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                train_data.to_csv(train_output_path, index=False)\n",
    "                test_data.to_csv(test_output_path, index=False)\n",
    "                print(f\"Saved training data to {train_output_path}\")\n",
    "                print(f\"Saved testing data to {test_output_path}\")\n",
    "\n",
    "                # Save validation data if it exists\n",
    "                if validation_data is not None:\n",
    "                    validation_output_path = os.path.join(output_folder, f\"validation_{selected_file.split('.')[0]}{cleaned_file_suffix}.csv\")\n",
    "                    validation_data.to_csv(validation_output_path, index=False)\n",
    "                    print(f\"Saved validation data to {validation_output_path}\")\n",
    "\n",
    "            # Generate and print encoding summary\n",
    "            encoding_summary = generate_encoding_summary(data, cleaned_data)\n",
    "            print_encoding_summary(encoding_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224bfd8-8c59-453e-b2f7-9e0f5d8fc79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
